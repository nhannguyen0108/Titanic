# -*- coding: utf-8 -*-
"""Titanic - Machine Learning from Disaster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17LHk7ZFMlrIT6isFjV8fcBdZAhupfNp_

***Project: Titanic - Machine Learning from Disaster
"""

#import Lib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

train_df.columns

test_df.columns

"""Notes:
*   SibSp: # of siblings / spouses aboard the Titanic
*   Parch: 	# of parents / children aboard the Titanic
*   Ticket: Ticket number
*   Cabin: Cabin number


"""

#preview data
train_df.head()

train_df.set_index(train_df.PassengerId, inplace=True)

train_df.head()

train_df.drop("PassengerId", axis =1, inplace=True)

train_df

test_df.head()

test_df.set_index(test_df.PassengerId, inplace=True)

test_df.head()

test_df.drop("PassengerId", axis = 1, inplace=True)

test_df

"""**1. Feature Classification: Categorical(Phân loại) vs Numerical(Số)**
* This helps us select the appropriate plots for visualization.

  Which features are categorical?
  * Categorical Features: nominal, ordinal, ratio, interval
  * To classify the samples into sets of similar samples

  Which features are numerical?
  * Numerical features: discrete, continuous or timeseries
  * These values change from sample to sample


"""

train_df.info()

test_df.info()

"""*   Categorical: Survived, Sex, Embarked, Pclass(ordinal), SibSp, Parch
*   Numerical: (continuous) Age, Fare (discrete)
*   Mix types of data: Ticket, Cabin
*   Contain Error/Typo: Name
*   Blank or Null: Cabin > Age > Embarked
*   Various Data Type: String, Int, Float



"""

train_df['Survived']= train_df['Survived'].astype('category')

train_df['Survived'].dtype

train_df.info()

features = {"Pclass", "Sex", "SibSp", "Parch", "Embarked"}
def convert_cat(df, features):
  for feature in features:
    df[feature] = df[feature].astype("category")
convert_cat(train_df, features)
convert_cat(test_df, features)

train_df.info()

"""**Distribution of Numerical feature values across the samples**"""

train_df.describe()

"""**Distibution of Categorical feature values across the sample**"""

train_df.describe(include=['category'])

"""3. Exploring Data Analysis (EDA)
- Correlation categorical features
  *   Categorical: Survived, Sex, Embarked, Pclass (ordinal), SibSp, Parch

- Target Variable: Survived



"""

train_df["Survived"].value_counts().to_frame()

train_df["Survived"].value_counts(normalize = True).to_frame()

"""Only 38% survived the disaster. So the training data suffers from data imbalance but it is not severe which is why I will not consider techniques like sampling to tackle the imbalance.

###'Sex'

"""

train_df['Sex'].value_counts().to_frame()

sns.countplot(data=train_df, x='Sex', hue='Survived', palette ='Blues');

"""- Remaining Categorical Feature Columns"""

cols = ["Pclass", "Sex", "SibSp", "Parch", "Embarked"]

n_rows = 2
n_cols = 3

fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols*3.5, n_rows*3.5))

for r in range(0, n_rows):
  for c in range(0, n_cols):
    i = r*n_cols + c #index to loop through list 'cols'
    if i < len(cols):
      ax_i = ax[r,c]
      sns.countplot(data=train_df, x=cols[i], hue='Survived', palette='Blues', ax=ax_i)
      ax_i.set_title(f'Figure {i+1}: Survived Rate vs {cols[i]}')
      ax_i.legend(title='', loc='upper right', labels=['Not Survived', 'Survivied'])
ax.flat[-1].set_visible(False) #Remove the last subplot
plt.tight_layout()
plt.show()

"""Observation:
* Survival Rate:
  * Fig 1: 1st class higher survival rate
  * Fig 2: Female highrt survival rate
  * Fig 3: People going with 0 Siblings are mostly not survived, the number of passenger with 1-2 family memers has a better chance of survival
  * Fig 4: People going with 0 Parent + Children are mostly not survival
  * Fig 5: Most people embarked at Southamption, and also had the highest people not survived

EDA for Numerical Features

*** Numerical features (continuous) Age, Fare***


***Age***
"""

sns.histplot(data=train_df, x="Age", hue="Survived", bins = 40, kde=True);

"""* Marjority passengers were from 18-40 ages.
* Children had more chance to survive than other ages.

***Fare***
"""

train_df["Fare"].describe()

sns.histplot(data=train_df, x="Fare",hue = "Survived", bins = 40, kde=True)

#To name for 0-25% quartile, 25-30, 50-75, 75-100

fare_categories = ["Economic", "Standard", "Second-Class", "First-Class"]
quartitle_data = pd.qcut(train_df["Fare"], 4, labels=fare_categories)

sns.countplot(x=quartitle_data, hue =train_df["Survived"], palette="Blues")
plt.show()

"""* Distribution of Fare
  *   Fare does not follow a normal distribution and has a huge spike at the price rang [0-$100].
  
  *   The distribution is skewed to the left with 75% of the fare paid under $31

and a max paid fare of $512
* Quartitle plot:
  * Passenger with Luxury & Expensive Fare will have more chance to survive

***4. Feature Engineering & Data Wrangling***

Name
- Regular Expression
"""

train_df["Name"].tail(10)

import re #regular expression
def extract_title(name):
  p = re.compile(r", ([\w\s]+)\.")
#   if p.search(name) is None:
#   print(name)
  return p.search(name).groups(1)[0].strip()

train_df['Title'] = train_df['Name'].apply(lambda name: extract_title(name))

train_df['Title'].value_counts()

test_df['Title'] = test_df['Name'].apply(lambda name:extract_title(name))

test_df['Title'].value_counts()

def group_title(title):
  if title in ['Mr', 'Mrs', 'Miss', 'Master']:
    return title
  elif title == 'Ms':
    return 'Miss'
  else:
    return 'Others'

train_df['Title'] = train_df['Title'].apply(lambda title: group_title(title))
test_df['Title'] = test_df['Title'].apply(lambda title: group_title(title))

sns.countplot(data=train_df, x='Title', hue='Survived');

"""***#Family***
***SubSp, Parch
"""

train_df['Family_Size'] = train_df['SibSp'].astype('int') +train_df['Parch'].astype('int') +1

test_df['Family_Size'] = test_df['SibSp'].astype('int') +test_df['Parch'].astype('int') +1

train_df['Family_Cat'] = pd.cut(train_df['Family_Size'], bins=[0,1,4,6,20], labels=['Solo', 'Small', 'Medium', 'Large']) #(0,1], (1,4], (4, 6], (6, 20]
test_df['Family_Cat'] = pd.cut(test_df['Family_Size'], bins=[0,1,4,6,20], labels=['Solo', 'Small', 'Medium', 'Large']) #(0,1], (1,4], (4, 6], (6, 20]

sns.countplot(data=train_df, x='Family_Cat', hue='Survived');

"""4.2 Data Wrangling"""

num_features = ['Age', 'Fare']
cat_features = ['Sex', 'Pclass', 'Embarked', 'Title', 'Family_Cat']
feature_cols = num_features + cat_features
print(feature_cols)

def display_missing(df, feature_cols):
  n_rows = df.shape[0]
  for col in feature_cols:
    missing_count = df[col].isnull().sum()
    if missing_count > 0:
      print(f"{col} has {missing_count*100/n_rows: .2f}% missing values")

display_missing(train_df, feature_cols)
display_missing(test_df, feature_cols)

"""###Filling missing values
### Age
- Filling missing values with median of whole dataset
"""

age_by_pclass_sex = train_df.groupby(["Sex","Pclass"],observed=False)['Age'].median()

age_by_pclass_sex

#Filling the missing values in Age with the medians of Sex and Pclass groups
train_df['Age'] = train_df.groupby(['Sex', 'Pclass'], observed = False)['Age'].transform(lambda x: x.fillna(x.median()))

test_df['Age'] = test_df.groupby(['Sex', 'Pclass'], observed = False)['Age'].transform(lambda x: x.fillna(x.median()))

display_missing(train_df, feature_cols)
display_missing(test_df, feature_cols)

X = train_df[feature_cols]
y = train_df['Survived']

X_test = test_df[feature_cols]

# preprocess Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

num_transformer = Pipeline(steps = [
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())

])

cat_transformer = Pipeline(steps = [
    ('Imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', num_transformer, num_features),
    ('cat', num_transformer, cat_features)
])

preprocessor.fit(X)

X = preprocessor.transform(X) #x->X_train, X_val

X_test = preprocessor.transform(X_test)

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

